{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b289f70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3496751",
   "metadata": {},
   "source": [
    "# Spam dataset was used for crime detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "158aa275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2 Unnamed: 2  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\HP\\OneDrive\\Documents\\isro\\spam.csv\",encoding='latin1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73b3ab30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5572</td>\n",
       "      <td>5572</td>\n",
       "      <td>50</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>5169</td>\n",
       "      <td>43</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>ham</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "      <td>bt not his girlfrnd... G o o d n i g h t . . .@\"</td>\n",
       "      <td>MK17 92H. 450Ppw 16\"</td>\n",
       "      <td>GNT:-)\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4825</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          v1                      v2  \\\n",
       "count   5572                    5572   \n",
       "unique     2                    5169   \n",
       "top      ham  Sorry, I'll call later   \n",
       "freq    4825                      30   \n",
       "\n",
       "                                               Unnamed: 2  \\\n",
       "count                                                  50   \n",
       "unique                                                 43   \n",
       "top      bt not his girlfrnd... G o o d n i g h t . . .@\"   \n",
       "freq                                                    3   \n",
       "\n",
       "                   Unnamed: 3 Unnamed: 4  \n",
       "count                      12          6  \n",
       "unique                     10          5  \n",
       "top      MK17 92H. 450Ppw 16\"    GNT:-)\"  \n",
       "freq                        2          2  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59909ac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['v1', 'v2', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2558179",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([ 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'],axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a97eb1dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd7a1499",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['v1_nm']=df.v1.map({'ham':0,'spam':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa97e802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>v1_nm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2  v1_nm\n",
       "0   ham  Go until jurong point, crazy.. Available only ...      0\n",
       "1   ham                      Ok lar... Joking wif u oni...      0\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...      1\n",
       "3   ham  U dun say so early hor... U c already then say...      0\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...      0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37959b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   v1      5572 non-null   object\n",
      " 1   v2      5572 non-null   object\n",
      " 2   v1_nm   5572 non-null   int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 130.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "664acec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     v1                                                 v2  v1_nm    class\n",
      "0   ham  Go until jurong point, crazy.. Available only ...      0  Quality\n",
      "1   ham                      Ok lar... Joking wif u oni...      0  Quality\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...      1  Quality\n",
      "3   ham  U dun say so early hor... U c already then say...      0  Quality\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...      0  Quality\n"
     ]
    }
   ],
   "source": [
    "\n",
    "attack_keywords = [\"gun\", \"kill\", \"assassin\", \"murder\", \"attack\", \"assault\", \"violence\"]\n",
    "def classify_text(text):\n",
    "    for keyword in attack_keywords:\n",
    "        if keyword.lower() in text.lower() or keyword.upper() in text.upper():\n",
    "            return \"Attack\"\n",
    "    return \"Quality\"  \n",
    "df['class'] = df['v2'].apply(classify_text)\n",
    "\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0183b759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your gonna have to pick up a $1 burger for yourself on your way home. I can't even move. Pain is killing me.\n",
      "Wife.how she knew the time of murder exactly\n",
      "I sent my scores to sophas and i had to do secondary application for a few schools. I think if you are thinking of applying, do a research on cost also. Contact joke ogunrinde, her school is one me the less expensive ones\n",
      "SMS. ac JSco: Energy is high, but u may not know where 2channel it. 2day ur leadership skills r strong. Psychic? Reply ANS w/question. End? Reply END JSCO\n",
      "U can WIN å£100 of Music Gift Vouchers every week starting NOW Txt the word DRAW to 87066 TsCs www.ldew.com SkillGame,1Winaweek, age16.150ppermessSubscription\n",
      "U can WIN å£100 of Music Gift Vouchers every week starting NOW Txt the word DRAW to 87066 TsCs www.Idew.com SkillGame, 1Winaweek, age16. 150ppermessSubscription\n",
      "Hey cutie. How goes it? Here in WALES its kinda ok. There is like hills and shit but i still avent killed myself. \n",
      "Haha, my legs and neck are killing me and my amigos are hoping to end the night with a burn, think I could swing by in like an hour?\n",
      "I don't run away frm u... I walk slowly &amp; it kills me that u don't care enough to stop me...\n",
      "Solve d Case : A Man Was Found Murdered On  &lt;DECIMAL&gt; . &lt;#&gt;  AfterNoon. 1,His wife called Police. 2,Police questioned everyone. 3,Wife: Sir,I was sleeping, when the murder took place. 4.Cook: I was cooking. 5.Gardener: I was picking vegetables. 6.House-Maid: I went 2 d post office. 7.Children: We went 2 play. 8.Neighbour: We went 2 a marriage. Police arrested d murderer Immediately. Who's It? Reply With Reason, If U r Brilliant.\n",
      "What's happening with you. Have you gotten a job and have you begun registration for permanent residency\n",
      "She is our sister.. She belongs 2 our family.. She is d hope of tomorrow.. Pray 4 her,who was fated 4 d Shoranur train incident. Lets hold our hands together &amp; fuelled by love &amp; concern prior 2 her grief &amp; pain. Pls join in dis chain &amp; pass it. STOP VIOLENCE AGAINST WOMEN.\n",
      "Solve d Case : A Man Was Found Murdered On  &lt;DECIMAL&gt; . &lt;#&gt;  AfterNoon. 1,His wife called Police. 2,Police questioned everyone. 3,Wife: Sir,I was sleeping, when the murder took place. 4.Cook: I was cooking. 5.Gardener: I was picking vegetables. 6.House-Maid: I went 2 d post office. 7.Children: We went 2 play. 8.Neighbour: We went 2 a marriage. Police arrested d murderer Immediately. Who's It? Reply With Reason, If U r Brilliant.\n",
      "House-Maid is the murderer, coz the man was murdered on  &lt;#&gt; th January.. As public holiday all govt.instituitions are closed,including post office..understand?\n",
      "House-Maid is the murderer, coz the man was murdered on  &lt;#&gt; th January.. As public holiday all govt.instituitions are closed,including post office..\n",
      "Wife.how she knew the time of murder exactly\n",
      "House-Maid is the murderer, coz the man was murdered on  &lt;#&gt; th January.. As public holiday all govt.instituitions are closed,including post office..understand?\n",
      "Wife.how she knew the time of murder exactly\n",
      "Solve d Case : A Man Was Found Murdered On  &lt;DECIMAL&gt; . &lt;#&gt;  AfterNoon. 1,His wife called Police. 2,Police questioned everyone. 3,Wife: Sir,I was sleeping, when the murder took place. 4.Cook: I was cooking. 5.Gardener: I was picking vegetables. 6.House-Maid: I went 2 d post office. 7.Children: We went 2 play. 8.Neighbour: We went 2 a marriage. Police arrested d murderer Immediately. Who's It? Reply With Reason, If U r Brilliant.\n",
      "I don't run away frm u... I walk slowly &amp; it kills me that u don't care enough to stop me...\n",
      "I just saw ron burgundy captaining a party boat so yeah\n",
      "We know TAJ MAHAL as symbol of love. But the other lesser known facts 1. Mumtaz was Shahjahan's 4th wife, out of his 7 wifes. 2. Shahjahan killed Mumtaz's husband to marry her. 3. Mumtaz died in her  &lt;#&gt; th delivery. 4. He then married Mumtaz's sister. Question arises where the Hell is the LOVE?:-| -The Great Hari-\n",
      "\\The world suffers a lot... Not because of the violence of bad people. But because of the silence of good people!\\\"\n",
      "U can WIN å£100 of Music Gift Vouchers every week starting NOW Txt the word DRAW to 87066 TsCs www.Idew.com SkillGame, 1Winaweek, age16. 150ppermessSubscription\n",
      "Kind of. Just missed train cos of asthma attack, nxt one in half hr so driving in. not sure where to park.\n",
      "House-Maid is the murderer, coz the man was murdered on  &lt;#&gt; th January.. As public holiday all govt.instituitions are closed,including post office..understand?\n",
      "Nt only for driving even for many reasons she is called BBD..thts it chikku, then hw abt dvg cold..heard tht vinobanagar violence hw is the condition..and hw ru ? Any problem?\n",
      "Don't worry though, I understand how important it is that I be put in my place with a poorly thought out punishment in the face of the worst thing that has ever happened to me. Brb gonna go kill myself\n"
     ]
    }
   ],
   "source": [
    "attack_texts = df[df['class'] == 'Attack']['v2']\n",
    "\n",
    "for text in attack_texts:\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5565dcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['class'] = df['class'].map({'Quality': 0, 'Attack': 1})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a910f173",
   "metadata": {},
   "source": [
    "# Replacing emojis with text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5439698a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in c:\\users\\hp\\anaconda3\\lib\\site-packages (2.10.1)\n",
      "     v1                                                 v2  v1_nm  class\n",
      "0   ham  Go until jurong point, crazy.. Available only ...      0      0\n",
      "1   ham                      Ok lar... Joking wif u oni...      0      0\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...      1      0\n",
      "3   ham  U dun say so early hor... U c already then say...      0      0\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...      0      0\n"
     ]
    }
   ],
   "source": [
    "!pip install emoji\n",
    "\n",
    "import emoji\n",
    "def replace_emojis_with_text(text):\n",
    "    return emoji.demojize(text)\n",
    "\n",
    "df['v2'] = df['v2'].apply(replace_emojis_with_text)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a611d5",
   "metadata": {},
   "source": [
    "# REPLACING URL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8cc59cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     v1                                                 v2  v1_nm  class\n",
      "0   ham  Go until jurong point, crazy.. Available only ...      0      0\n",
      "1   ham                      Ok lar... Joking wif u oni...      0      0\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...      1      0\n",
      "3   ham  U dun say so early hor... U c already then say...      0      0\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...      0      0\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def replace_urls_with_url(text):\n",
    "    # Regular expression pattern to match URLs\n",
    "    url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    # Replace URLs with \"URL\"\n",
    "    return re.sub(url_pattern, \"URL\", text)\n",
    "df['v2'] = df['v2'].apply(replace_urls_with_url)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647a539d",
   "metadata": {},
   "source": [
    "# REMOVING PUNCTUATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60664021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     v1                                                 v2  v1_nm  class\n",
      "0   ham  Go until jurong point crazy Available only in ...      0      0\n",
      "1   ham                            Ok lar Joking wif u oni      0      0\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...      1      0\n",
      "3   ham        U dun say so early hor U c already then say      0      0\n",
      "4   ham  Nah I dont think he goes to usf he lives aroun...      0      0\n"
     ]
    }
   ],
   "source": [
    "def remove_punctuations(text):\n",
    "    # Define regular expression pattern to match punctuations\n",
    "    punctuation_pattern = r'[^\\w\\s]'  # Matches any character that is not a word character or whitespace\n",
    "    # Remove punctuations using regex\n",
    "    return re.sub(punctuation_pattern, '', text)\n",
    "\n",
    "\n",
    "df['v2'] = df['v2'].apply(remove_punctuations)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a65383",
   "metadata": {},
   "source": [
    "# TOKENIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5932457f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     v1                                                 v2  v1_nm  class  \\\n",
      "0   ham  Go until jurong point crazy Available only in ...      0      0   \n",
      "1   ham                            Ok lar Joking wif u oni      0      0   \n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...      1      0   \n",
      "3   ham        U dun say so early hor U c already then say      0      0   \n",
      "4   ham  Nah I dont think he goes to usf he lives aroun...      0      0   \n",
      "\n",
      "                                              tokens  \n",
      "0  [go, until, jurong, point, crazy, available, o...  \n",
      "1                     [ok, lar, joking, wif, u, oni]  \n",
      "2  [free, entry, in, 2, a, wkly, comp, to, win, f...  \n",
      "3  [u, dun, say, so, early, hor, u, c, already, t...  \n",
      "4  [nah, i, dont, think, he, goes, to, usf, he, l...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt') \n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "df['tokens'] = df['v2'].apply(tokenize_text)\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52104a4",
   "metadata": {},
   "source": [
    "# REMOVING STOP WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02c4a504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     v1                                                 v2  v1_nm  class  \\\n",
      "0   ham  Go until jurong point crazy Available only in ...      0      0   \n",
      "1   ham                            Ok lar Joking wif u oni      0      0   \n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...      1      0   \n",
      "3   ham        U dun say so early hor U c already then say      0      0   \n",
      "4   ham  Nah I dont think he goes to usf he lives aroun...      0      0   \n",
      "\n",
      "                                              tokens  \n",
      "0  [go, jurong, point, crazy, available, bugis, n...  \n",
      "1                     [ok, lar, joking, wif, u, oni]  \n",
      "2  [free, entry, 2, wkly, comp, win, fa, cup, fin...  \n",
      "3      [u, dun, say, early, hor, u, c, already, say]  \n",
      "4  [nah, dont, think, goes, usf, lives, around, t...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def rm_stop_words(tokens):\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_tokens\n",
    "df['tokens'] = df['tokens'].apply(rm_stop_words)\n",
    "print(df.head())   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5112529",
   "metadata": {},
   "source": [
    "# STEMMING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d56e05f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     v1                                                 v2  v1_nm  class  \\\n",
      "0   ham  Go until jurong point crazy Available only in ...      0      0   \n",
      "1   ham                            Ok lar Joking wif u oni      0      0   \n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...      1      0   \n",
      "3   ham        U dun say so early hor U c already then say      0      0   \n",
      "4   ham  Nah I dont think he goes to usf he lives aroun...      0      0   \n",
      "\n",
      "                                              tokens  \n",
      "0  [go, jurong, point, crazi, avail, bugi, n, gre...  \n",
      "1                       [ok, lar, joke, wif, u, oni]  \n",
      "2  [free, entri, 2, wkli, comp, win, fa, cup, fin...  \n",
      "3      [u, dun, say, earli, hor, u, c, alreadi, say]  \n",
      "4  [nah, dont, think, goe, usf, live, around, tho...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('punkt') \n",
    "def stem_tokens(tokens):\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return stemmed_tokens\n",
    "df['tokens'] = df['tokens'].apply(stem_tokens)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a681fcb2",
   "metadata": {},
   "source": [
    "# LEMMENIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "668ab802",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     v1                                                 v2  v1_nm  class  \\\n",
      "0   ham  Go until jurong point crazy Available only in ...      0      0   \n",
      "1   ham                            Ok lar Joking wif u oni      0      0   \n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...      1      0   \n",
      "3   ham        U dun say so early hor U c already then say      0      0   \n",
      "4   ham  Nah I dont think he goes to usf he lives aroun...      0      0   \n",
      "\n",
      "                                              tokens  \n",
      "0  [go, jurong, point, crazi, avail, bugi, n, gre...  \n",
      "1                       [ok, lar, joke, wif, u, oni]  \n",
      "2  [free, entri, 2, wkli, comp, win, fa, cup, fin...  \n",
      "3      [u, dun, say, earli, hor, u, c, alreadi, say]  \n",
      "4  [nah, dont, think, goe, usf, live, around, tho...  \n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt') \n",
    "nltk.download('wordnet')\n",
    "def lemmatize_tokens(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return lemmatized_tokens\n",
    "df['tokens'] = df['tokens'].apply(lemmatize_tokens)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beec3440",
   "metadata": {},
   "source": [
    "# CONVERTING WORD TO SENTENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "721d7ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     v1                                                 v2  v1_nm  class  \\\n",
      "0   ham  Go until jurong point crazy Available only in ...      0      0   \n",
      "1   ham                            Ok lar Joking wif u oni      0      0   \n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...      1      0   \n",
      "3   ham        U dun say so early hor U c already then say      0      0   \n",
      "4   ham  Nah I dont think he goes to usf he lives aroun...      0      0   \n",
      "\n",
      "                                              tokens  \\\n",
      "0  [go, jurong, point, crazi, avail, bugi, n, gre...   \n",
      "1                       [ok, lar, joke, wif, u, oni]   \n",
      "2  [free, entri, 2, wkli, comp, win, fa, cup, fin...   \n",
      "3      [u, dun, say, earli, hor, u, c, alreadi, say]   \n",
      "4  [nah, dont, think, goe, usf, live, around, tho...   \n",
      "\n",
      "                                            sentence  \n",
      "0  go jurong point crazi avail bugi n great world...  \n",
      "1                              ok lar joke wif u oni  \n",
      "2  free entri 2 wkli comp win fa cup final tkt 21...  \n",
      "3                u dun say earli hor u c alreadi say  \n",
      "4          nah dont think goe usf live around though  \n"
     ]
    }
   ],
   "source": [
    "def words_to_sentence(word_list):\n",
    "    return ' '.join(word_list)\n",
    "df['sentence'] = df['tokens'].apply(words_to_sentence)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4addd5",
   "metadata": {},
   "source": [
    "# TF-IDF VECTORIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "471467ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   008704050406  0089mi  0121  01223585236  01223585334  0125698789   02  \\\n",
      "0           0.0     0.0   0.0          0.0          0.0         0.0  0.0   \n",
      "1           0.0     0.0   0.0          0.0          0.0         0.0  0.0   \n",
      "2           0.0     0.0   0.0          0.0          0.0         0.0  0.0   \n",
      "3           0.0     0.0   0.0          0.0          0.0         0.0  0.0   \n",
      "4           0.0     0.0   0.0          0.0          0.0         0.0  0.0   \n",
      "\n",
      "   020603  0207  02070836089  ...   ì_  ì_ll  ìcopyright   ìï  ìïll   û_  \\\n",
      "0     0.0   0.0          0.0  ...  0.0   0.0         0.0  0.0   0.0  0.0   \n",
      "1     0.0   0.0          0.0  ...  0.0   0.0         0.0  0.0   0.0  0.0   \n",
      "2     0.0   0.0          0.0  ...  0.0   0.0         0.0  0.0   0.0  0.0   \n",
      "3     0.0   0.0          0.0  ...  0.0   0.0         0.0  0.0   0.0  0.0   \n",
      "4     0.0   0.0          0.0  ...  0.0   0.0         0.0  0.0   0.0  0.0   \n",
      "\n",
      "   û_thank   ûï  ûïharri   ûò  \n",
      "0      0.0  0.0      0.0  0.0  \n",
      "1      0.0  0.0      0.0  0.0  \n",
      "2      0.0  0.0      0.0  0.0  \n",
      "3      0.0  0.0      0.0  0.0  \n",
      "4      0.0  0.0      0.0  0.0  \n",
      "\n",
      "[5 rows x 8032 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['sentence'])\n",
    "t_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "print(t_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a2021c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_df['class'] = df['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea69d77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['008704050406', '0089mi', '0121', '01223585236', '01223585334',\n",
      "       '0125698789', '02', '020603', '0207', '02070836089',\n",
      "       ...\n",
      "       'ì_', 'ì_ll', 'ìcopyright', 'ìï', 'ìïll', 'û_', 'û_thank', 'ûï',\n",
      "       'ûïharri', 'ûò'],\n",
      "      dtype='object', length=8032)\n"
     ]
    }
   ],
   "source": [
    "print(t_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "02b7705c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 1.0\n",
      "KNN Accuracy: 1.0\n",
      "Naive Bayes Accuracy: 0.9946172248803827\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(t_df, df['class'], test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "svm_classifier = SVC()\n",
    "knn_classifier = KNeighborsClassifier()\n",
    "naive_bayes_classifier = MultinomialNB()\n",
    "\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "knn_classifier.fit(X_train, y_train)\n",
    "naive_bayes_classifier.fit(X_train, y_train)\n",
    "\n",
    "svm_predictions = svm_classifier.predict(X_test)\n",
    "knn_predictions = knn_classifier.predict(X_test)\n",
    "naive_bayes_predictions = naive_bayes_classifier.predict(X_test)\n",
    "\n",
    "\n",
    "svm_accuracy = accuracy_score(y_test, svm_predictions)\n",
    "knn_accuracy = accuracy_score(y_test, knn_predictions)\n",
    "naive_bayes_accuracy = accuracy_score(y_test, naive_bayes_predictions)\n",
    "\n",
    "print(\"SVM Accuracy:\", svm_accuracy)\n",
    "print(\"KNN Accuracy:\", knn_accuracy)\n",
    "print(\"Naive Bayes Accuracy:\", naive_bayes_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37815372",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
